\documentclass[11pt,a4paper]{report}
%\usepackage{setspace}
%\onehalfspacing 
\usepackage[square,numbers,comma,sort&compress]{natbib}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{float}
%\usepackage{hyperref}
%\usepackage[nottoc]{tocbibind}
\begin{document}
\title{An explanation of the paper -\\``Neural fingerprint generation of molecules"}
\author{Varun Kumar\\ BITS-Pilani, K.K. Birla Goa Campus}
\maketitle
\tableofcontents
\pagebreak

\section{Introduction}
This report explains the working of the code from the paper "Convolutional Networks on Graphs for Learning Molecular Fingerprints". Though the title of the paper says "Convolution Networks", the network that thay have constructed is quite different from the traditional Covolution Networks which we have come across while working with images or audio signals. Author calls it convolution network because same operation is performed over all the atoms in a molecule, this is similar to using the same filter on multiple places of the image. As the next layer of the convolutional network gets a larger receptive field of the image as compared to the previous layer, here also, with every new layer(or radius) the receptive field of the graph on the atoms is increased. I have considered the molecular package RDKit used in the paper as a black box. It was used to construct graph from SMILES representation of molecules and extract the handcrafted features of atoms and bonds. They formulated the problem of extracting fixed sized features as a regression problem for predicting solubility which generates these fixed sized features in between.

\section{Dataset}
They have used the solubility dataset \citep{delaney2004esol} to predict solubility. SMILES representation of molecules were used for training with `measured solubility in moles per litre' as target labels. A snapshot of the dataset used is shown in the Table~\ref{dataset}.
Each SMILE converted to a graph with the nodes representing atoms and edges representing bonds. Handcrafted features are extracted from each node and edge and is called \textit{atom feature} and \textit{bond feature} respectively. A tensor is created of these atom features for all the nodes of graph of the molecules taken in the training set/minibatch. This \textit{input tensor} of size (\#Nodes, length(atom features)) is used as the input to neural network. Hence each atom becomes a unique node i.e. the same carbon atom present in different molecules will be referred to as different nodes.

\begin{table}[H]
\centering
\caption{Snapshot of dataset used}
\label{dataset}
\begin{tabular}{|l|l|}
\hline
\textbf{Smiles}                                        & \textbf{Solubility} \\ \hline
OCC3OC(OCC2OC(OC(C\#N)c1ccccc1)C(O)C(O)C2O)C(O)C(O)C3O & -0.77                                              \\ \hline
Cc1occc1C(=O)Nc2ccccc2                                 & -3.3                                               \\ \hline
CC(C)=CCCC(C)=CC(=O)                                   & -2.06                                              \\ \hline
c1ccc2c(c1)ccc3c2ccc4c5ccccc5ccc43                     & -7.87                                               \\ \hline
c1ccsc1                                                & -1.33                                              \\ \hline
\end{tabular}
\end{table}

\section{Architecture }
They created the architecture to extract a fixed size fingerprint of a molecule and then used it to predict its solubility using another neural network. In place of a full neural network a linear regressor can also be used (See figure~\ref{complete_arch}) for the task of prediction. They trained the network end to end thereby generating the features in between. I like to call this architecture a 2 way neural network with a horizontal pass and a vertical pass.

A 4 layered neural net is used for feature extraction. Layers are - [ Input\_Layer, Hidden\_Layer1, Hidden\_Layer2, Hidden\_Layer3 ]. The layer and radius is used interchangably in this report.
%Each SMILES representation of a molecule is first converted to a graph using RDKit. The nodes of graph are considered as atoms and edges as bonds. And then handcrafted atom features and bond features are extracted using RDKit. The tensor of all atom features are used as input to the neural network aka neural fingerprint network.
\textit{Input tensor} is forward propogated via these layers described above. - I call this a horizontal pass. The reason behind forward propogating the whole tensor at once is that after every layer you need to update the features of all the atom nodes in the tensor together. Because at each update, the atom features are summed up with the neighbouring atom features i.e. after every layer/radius the atom node contains a representation of its neighbouring atom nodes as well. Therefore it is manditory that all the neighbouring nodes have the features of the same layer.

Now you may wonder where is the output layer. There is no single output layer like a traditional neural network has. Instead, output is calculated at each layer (including the Input\_Layer). - I call this the vertical pass. 

\begin{figure}[H]
	\centering
	\caption{A high level view of complete architecture}
	\label{complete_arch}
	\includegraphics[scale=0.50]{complete_arch.png}
\end{figure}

\section{Methodology}

\textbf{Horizontal Pass}:\\

In horizontal pass, outputs of the previous layer is updated and tranformed as inputs of the next layer. So there are 3 Such transformations as per the architecture shown in the previous section. Let us look at what happens at each transformation.
 
 \begin{itemize}
 \item At each layer, the atom features of previous layer is first transformed by some weight matrix aka \textit{self filter} (which changes the size of the features to size of the next layer), resulting in \textit{self activations}.
 
 \item These \textit{self activations} are summed up with the transformed features of its neighbouring atoms and bonds.
 
 \item Now, to find the transformed features of neighbours for each atom node, first the neighboring atom nodes and corresponding bonds of each atom node is found. Then their feature vectors are found using RDKit. These feature vectors are then concatenated. 
 
 \item These concatenated features are again transformed by another weight matrix aka \textit{filter} to give \textit{activations by degree}. Degree of an atom is defined to be its number of directly-bonded neighbors. As any atom can have maximum of 5 degrees (for organic molecules), we need 5 different \textit{fiters}. 
 
 \item Another point to note here is that the list of atoms is sorted according to the degree of atoms. The top atom in the list is the atom of degree 0 or 1 and the last atom is of maximum degree of all the atoms in the dataset. For each degree corresponding set of atoms are selected and their neighbours are found out. Then the neighbours belonging to a particular degree atom is multiplied by a particular \textit{filter} to get \textit{activations by degree} as mentioned in the previous point. This is done for all the degrees. The results are then appended one after another to get the transformed neighbour activations for each atom in the list.
 
 \item As mentioned eatlier in the second point, these transformed neighbour activations are then added to the \textit{self activations} to get the summed activations. 
 
 \item These summed activations are then passed through an activation function (here relu) to get the final activations to be fed to the next layer. 
 
 \end{itemize}
 
 \textbf{Vertical Pass:}
 \begin{itemize}
 \item For vertical pass, input of the current layer is multiplied by output weight matrix aka \textit{layer output weights} and is passed through a softmax which results in a fixed size vector. This results in the blue tensors shown in the Figure~\ref{neural_fp}.
 
 \item After multiplyig weight matrix with the set of atom features and getting a huge matrix of size (\#atoms, FP\_size) i.e. blue tensors, we then sum up the subsets of atom activations that belong to each seperate molecule. This is called \textit{sum\_and\_stack} operation. This results in a grey tensor of size (\#molecules, FP\_size). We get this type of different grey tensors for each layer (including the Input\_Layer).
 
 \item Next, we sum up these tensors/matrices from each layer to get a single Fingerprint matrix of size (\#molecules, FP\_size) - red tensor. This is our final fingerprint output which we are going to improve later by updating the weights of the``convolution" aka fingerprint network by gradient descent.
 
 \end{itemize}
 
 So, to summarize, we can say that in total we deal with 3 types of weight matrices.
 
\begin{itemize}
\item Self filter (One for each transformation among layers)
\item Filter (One for each degree)
\item Output Weight matrix (One for each layer including the Input\_Layer)


\end{itemize}



%It is done by finding the neighboring atoms and bonds of each atom, finding their feature vectors and then concatenating them. These concatenated features are again transformed by another weight matrix aka \textit{filter} to give \textit{activations by degree}. Degree of an atom is defined to be its number of directly-bonded neighbors. As any atom can have maximum of 5 degrees (for organic molecules), we need 5 different \textit{fiters}. Atoms are sorted according to the degrees. The top atom is the atom of degree 0 or 1 and the last atom is of maximum degree of all the atoms in the dataset. For each degree some set of atoms will be selected and their neighbours will be found out. Then the neighbours belonging to a particular degree will be multiplied by a particular \textit{filter} to get \textit{activations by degree} for that particular degree. This is done for all the degrees. 
%
%For vertical pass, input of the current layer is multiplied by output weight matrix aka \textit{layer output weights} which results in a fixed size vector.
%
%After multiplyig weight matrix with the set of atom features and getting a huge matrix of size (\#atoms, FP\_size), we then sum up the subsets of atom activations that belong to a seperate molecule. This results in a matrix of size (\#molecules, FP\_size). We get this type of different matrix for each layer (including the Input\_Layer). Next, sum up these matrices from each layer to get a single Fingerprint matrix of size (\#molecules, FP\_size). This is our final fingerprint output which we are going to improve later by updating the weights of the "convolution" aka fingerprint network by gradient descent.\\
%
%So, in total we deal with 3 types of weight matrices.
%
%\begin{itemize}
%\item Self filter (One for each transformation among layers)
%\item Filter (One for each degree)
%\item Output Weight matrix (One for each layer including the Input\_Layer)
%
%
%\end{itemize}

This fingerprint is treated as input to a linear regression module or another neural network to predict solubility. The root mean squared error in solubility is backpropogated till the start of Fingerprint network to tune the weights of the complete network along with the weights of the linear regressor. The detailed view of fingerprint network that extracts fixed size fingerprints for each molecule can be seen in Figure~\ref{neural_fp}.

\begin{figure}[]
	\centering
	\caption{A detailed level view of Fingerprint Network}
	\label{neural_fp}
	\includegraphics[scale=.50]{neural_fp.png}
\end{figure}

\section{Visualization	}
This section describes about the intrepretability of the features learned. To visualize which atoms contriubute most towards solubility, they tried to find out the atoms that have the highest activations among all the layers of the fingerprint network. Before \textit{sum\_and\_satck} operation they appended the blue tensors shown in the figure~\ref{neural_fp} into one long list. They then calculated, for each index of fingerprint, which atom for the the same index as of fingerprint, has the highest activation. Then highlighted that atom. The neighbours are also highlighted based on the radius of the atom under consideration. If radius is 0, the atom itself is highlighted. If radius is 1, the atom and its neighbours are highlighted. If radius is 2, then the atom, its neighbours and its neighbours' neighbours are highlighted and so on. Radius refers to the layer of neural net the atom with the highest activation belonged to.

% Before sum and stack, we actually append all the atom activations of all the layers in one list. It is from this list we are finding the maximally activated atoms and highlighting.

I think what they mean is that to get higher solubility, all the values of fingerprint should be high. For the values of fingerprint should be high, the corresponding atom activations should be high.

They then plotted the atoms and highlighted them that contributed highest to the solubility.

\bibliographystyle{unsrt}
\bibliography{ref}
\end{document}